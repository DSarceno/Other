# CÃ©dula de SupervisiÃ³n

Autores:
- Marcela CotÃ­ Pac
- Diego SarceÃ±o


## ConfiguraciÃ³n
Se recomienda crear un entorno virtual e instalar el ***requirements.txt*** para instalar los paquetes y sus dependencias.

```
$ python -m venv <nombre_del_entorno>
```

Esto se recomienda hacerlo fuera del proyecto, para activarlo es sencillo:
```
$ C:\ruta\al\entorno\Scripts\activate
```

AdemÃ¡s, es Ãºtil tener el entorno virtual en el kernel de JupyterNotebook. Por ende es necesario realizar los siguiente:
```
python -m ipykernel install --user --name nombre_del_entorno --display-name "Python <nombre_del_entorno>"
```

Instalar el ***requirements.txt***:
```
$ pip install -r requirements.txt
```

AdemÃ¡s, para trabajar con la librerÃ­a de *nltk* (Natural Language Toolkit) es necesario instalar unas pequeÃ±as ayudas (***Corpus***, los cuales son conjuntos de datos, modelos o listas que se utilizan para el procesamiento de lenguaje natural), las cuales solo deben de realizarse la primera vez:
```
directorio = 'c:\\ruta\\al\\entorno\\lib\\nltk_data'

# Agrega la ruta de descarga a la lista de rutas de NLTK
nltk.data.path.append(directorio)

# Descargar el recurso 'punkt', 'punkt_tab', 'stopwords'
nltk.download('punkt', download_dir=directorio)
nltk.download('punkt_tab', download_dir=directorio)
nltk.download('stopwords', download_dir=directorio)
```


Estos que se instalaron tienen las siguientes caracterÃ­sticas y usos:
1. **punkt:**
   - DescripciÃ³n: Es un modelo de tokenizaciÃ³n que permite dividir un texto en oraciones o palabras. Se basa en un algoritmo de aprendizaje automÃ¡tico que identifica los lÃ­mites de las oraciones.
   - Uso: Generalmente se utiliza para preprocesar texto antes de anÃ¡lisis mÃ¡s profundos, como el anÃ¡lisis de sentimiento o la extracciÃ³n de caracterÃ­sticas.
1. **punkt_tab:**
   - DescripciÃ³n: Similar a 'punkt', pero diseÃ±ado para manejar tabulaciones y formatos especÃ­ficos de texto que pueden incluir caracteres especiales.
   - Uso: Utilizado en casos donde el texto tiene un formato tabular o donde los delimitadores son diferentes a los tÃ­picos espacios y puntos.
2. **stopwords:**
   - DescripciÃ³n: Es una lista de palabras comunes (como "y", "el", "de", etc.) que generalmente se eliminan de los textos porque no aportan valor semÃ¡ntico significativo en muchas aplicaciones de procesamiento de lenguaje natural.
   - Uso: Se utiliza en el preprocesamiento de textos para mejorar la calidad de los modelos de anÃ¡lisis, eliminando palabras que pueden introducir ruido en el anÃ¡lisis.


Para la parte de *lematizaciÃ³n* instala spacy (se encuentra en el requirements), pero es necesario agregar el modelo en espaÃ±ol, este se instala de la siguiente manera:
```
$ python -m spacy download es_core_news_sm
```

## En este Repositorio

> ðŸ’¬ **NOTE:**
Luego del preprocesamiento de los datos, es necesario realizar un [anÃ¡lisis exploratorio](EDA.md) para tener una visiÃ³n mÃ¡s clara sobre con quÃ© estamos trabajando.

### Estructura del Repositorio

```markdown
C:\Users\daalvarado\OneDrive - ine.gob.gt (1)\Documentos\diego_sarceÃ±o\cedulasupervision/
â”œâ”€â”€ .env
â”œâ”€â”€ paquetes.txt
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ supervision/
    â”œâ”€â”€ data/
        â”œâ”€â”€ IMDB_spanish.csv
        â”œâ”€â”€ imdb_espaniol_mod.csv
        â”œâ”€â”€ netflix/
            â””â”€â”€ film_reviews_result.csv
    â”œâ”€â”€ main.ipynb
    â”œâ”€â”€ main.py
    â”œâ”€â”€ misc/
        â”œâ”€â”€ evaluacion.png
        â”œâ”€â”€ lib_config.txt
        â””â”€â”€ queries_info_sup_certificate.sql
    â”œâ”€â”€ model/
        â”œâ”€â”€ csModel.h5
        â”œâ”€â”€ csModel.weights.h5
        â”œâ”€â”€ cs_tokenizer.json
        â”œâ”€â”€ maxlen
        â”œâ”€â”€ my_model.h5
        â””â”€â”€ my_model.keras
    â”œâ”€â”€ notebooks/
        â”œâ”€â”€ Polars_v_Pandas.ipynb
        â”œâ”€â”€ amazon_books.ipynb
        â”œâ”€â”€ amazon_reviews.ipynb
        â”œâ”€â”€ eda.ipynb
        â”œâ”€â”€ netflix.ipynb
        â”œâ”€â”€ text_manipulation.ipynb
        â””â”€â”€ translaltor.ipynb
    â”œâ”€â”€ src/
        â”œâ”€â”€ __init__.py
        â”œâ”€â”€ data_preprocessing.py
        â”œâ”€â”€ model.py
        â””â”€â”€ utils.py
    â””â”€â”€ test/
        â””â”€â”€ model_tests.ipynb
â””â”€â”€ upload_gitlab_rep.bat
```



## DocumentaciÃ³n

### Sentiment Analysis:
El ***anÃ¡lisis de sentimientos*** es una tÃ©cnica dentro de la minerÃ­a de texto y el procesamiento de lenguaje natural (NLP) que tiene como objetivo identificar y clasificar las emociones o actitudes expresadas en un texto. Este anÃ¡lisis permite comprender si el tono del texto es **positivo, negativo o neutral**, y a veces incluso captar las emociones mÃ¡s especÃ­ficas como alegrÃ­a, enojo o tristeza.

Este se basa en el uso de algorimos y modelos de machine learning o deep learning, que son entrenados con grandes cantidades de datos para reconocer patrones ligÃ¼isticos y asociarlos con distintos sentimientos. TambiÃ©n puede usar diccionarios de palabras con sentimientos asociados, o bien modelos estadÃ­sticos avanzados que consideran el contexto para una interpretaciÃ³n mÃ¡s clara.


##### Objetivos principales del AnÃ¡lisis de Sentimientos

1. **ClasificaciÃ³n de sentimientos**: Identificar el tono general del texto, ya sea positivo, negativo o neutral.
2. **ComprensiÃ³n de emociones especÃ­ficas**: En algunos casos, identificar emociones particulares como miedo, sorpresa o enojo.
3. **AnÃ¡lisis de tendencias**: Ayudar a empresas y organizaciones a detectar patrones y tendencias en las opiniones de sus usuarios o clientes.
4. **Mejora en la toma de decisiones**: Facilitar que las empresas ajusten sus estrategias de marketing, servicio al cliente o desarrollo de productos segÃºn el feedback emocional de los usuarios.

#### ImplementaciÃ³n.
##### - Limpieza y Preprocesamiento de Datos
De los datasets descargados mostrados posteriormente, solamente se utilizÃ³ el primero mencionado, esto debido a que estÃ¡ en espaÃ±ol, tiene una buena cantidad de datos y es bastante simple su forma de mostrar los datos. Con esto se realizÃ³ el preprocesamiento y limpieza de los datos. 

En cuanto a la limpieza no hubo mucho que hacer, no habÃ­a irregularidades ni inconsistencias en los datos, asÃ­ como valores nulos. Sin embargo, en el preprocesamiento se realizaron diferentes pasos:

1. Se eliminaron las etiquetas html que pudiera tener. Algunos datasets las traen por la forma en la que son construidos y de donde se extrae la informaciÃ³n; en el caso de nuestro dataset fue mÃ¡s por precauciÃ³n que por otra cosa.
2. Se eliminan los caracteres especiales ya que estos terminan no aportando nada al texto.
3. Quitamos los espacios extra, tanto al inicio, como dentro del texto, como al final.
4. Eliminamos las llamadas *stopwords* (de, el, y, es, etc.) que tampoco aportan nada al contexto del mismo.
5. Se realiza la *lematizaciÃ³n* del texto, la cual consiste en llevar las palabras a su forma base, como los verbos a infinitivo.

Luego de realizar esto, ya nos trasladamos a la parte de crear y evaluar el modelo, separando la data.


##### - Modelo
Se separan los datos en 2 conjuntos, conjunto de entrenamiento y conjunto de pruebas, que a su vez se divide en 2 conjuntos, el de evaluaciÃ³n del modelo y el de validaciÃ³n del modelo. Ya con estos conjuntos hechos, procedemos a definir la arquitectura de nuestra red.

```
model = Sequential([
    Embedding(max_features, embed_size),   # Capa de Embedding: representa palabras en dimensiones vectoriales
    LSTM(60, return_sequences=True),       # Capa LSTM: modelo de secuencia que aprende patrones temporales y contextuales
    GlobalMaxPool1D(),                     # ReducciÃ³n de dimensionalidad manteniendo valores mÃ¡ximos de las secuencias
    Dense(50, activation='relu'),          # Capa Densa: aÃ±ade capacidad de aprendizaje no lineal
    Dropout(0.1),                          # Dropout: evita el sobreajuste desactivando neuronas de forma aleatoria
    Dense(2, activation='sigmoid')         # Capa de salida: activaciÃ³n sigmoide para clasificar entre dos clases (positivo y negativo)
])
```

Explicando cada capa:
- Embedding: Su funciÃ³n es similar a la de Word2Vec, que genera un vector de caracterÃ­sticas para cada palabra en el texto. En donde `max_features` es el tamaÃ±o de vocabulario y `embed_size` es la dimensionalidad del embeding (en este caso 128 dimensiones).
- LSTM (Long Short-Term Memory): Las LSTMs son redes neuronales recurrentes (RNN) especializadas para aprender patrones secuenciales. Capturan relaciones contextuales entre palabras de la reseÃ±a, lo cual es muy Ãºtil para entender el sentimiento en funciÃ³n de la estructura y contexto de las frases. Esta, en concreto, tiene 60 unidades y el parÃ¡metro `return_sequences=True` permite que la capa devuelva la secuencia completa, no solo la Ãºltima salida.
- GlobalMaxPool1D: Esta aplica una operaciÃ³n de max pooling en la secuencia, tomando el valor mÃ¡ximo de cada dimensiÃ³n. Esto reduce la secuencia de salida de LSTM a una sola capa densa y evita un crecimiento excesivo en el nÃºmero de parÃ¡metros.
- Dense (50 unidades): Es una capa densa con 50 neuronas y activaciÃ³n `relu`. AÃ±ade capacidad de apredizaje no lineal.
- Dropout (10%): Ayuda a evitar el sobreajuste al apagar aleatoriamente el 10% de las neuronas durante el entrenamiento.
- Output (Sigmoid, 2 clases): La capa de salida usa una activaciÃ³n sigmoid y devuelve dos valores, uno para cada clase (positivo y  negativo). Esto es adecuado para la clasificaciÃ³n binaria, donde se interpretan los valroes como probabilidades para cada clase.

Compilando el modelo: `model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
`
1. `loss='sparse_categorical_crossentropy'`: 
   `sparse_categorical_crossentropy` es una funciÃ³n de pÃ©rdida utilizada para problemas de clasificaciÃ³n en mÃºltiples clases (multiclase). Este tipo de pÃ©rdida es adecuado cuando las etiquetas de clase estÃ¡n en forma de enteros (por ejemplo, 0 y 1 para binario, o 0, 1, 2, ... para varias clases).
   La "sparsidad" significa que las etiquetas no estÃ¡n en formato de "one-hot encoding" (es decir, no estÃ¡n en un vector con ceros y un uno en la posiciÃ³n correspondiente a la clase); simplemente son enteros representando las clases. Esto simplifica el procesamiento y es especialmente Ãºtil cuando tienes muchas clases o un gran volumen de datos, ya que reduce el tamaÃ±o del tensor de etiquetas.
2. `optimizer='adam'`: 
   `adam` es un optimizador popular que combina las ventajas de dos mÃ©todos de optimizaciÃ³n: `AdaGrad` y `RMSProp`. Es adaptable y ajusta automÃ¡ticamente la tasa de aprendizaje durante el entrenamiento, lo que generalmente permite una convergencia rÃ¡pida y estable.
   Adam es ampliamente utilizado en deep learning, ya que funciona bien en una variedad de tareas y modelos.
3. `metrics=['accuracy']`: 
   AquÃ­ estamos utilizando `accuracy` (precisiÃ³n) como mÃ©trica para monitorear el rendimiento del modelo durante el entrenamiento y la validaciÃ³n. La precisiÃ³n mide la proporciÃ³n de predicciones correctas sobre el total de predicciones, lo cual es Ãºtil en problemas de clasificaciÃ³n.


Ajuste/entrenamiento del modelo:
```
early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)
model.fit(X_train, Y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_val, Y_val), callbacks=[early_stopping])

```

1. `X_train` y `Y_train`: 
   Estos son los datos de entrada y las etiquetas de entrenamiento, respectivamente.
   - `X_train` contiene las muestras de entrada (en este caso, embeddings de reseÃ±as o secuencias de palabras).
   - `Y_train` contiene las etiquetas de salida correspondientes (por ejemplo, 0 o 1 para anÃ¡lisis de sentimientos binario).
2. `batch_size=batch_size`: 
   `batch_size` es el nÃºmero de muestras que se procesan antes de que el modelo actualice sus pesos.
   - Un tamaÃ±o de lote mÃ¡s pequeÃ±o puede hacer que el modelo actualice sus parÃ¡metros mÃ¡s frecuentemente, pero puede ser mÃ¡s lento y menos estable.
   - Valores tÃ­picos de `batch_size` son 16, 32, 64, etc. (es recomendable que sean potencias de 2 por eficiencia en GPU).
3. `epochs=epochs`: 
   `epochs` es el nÃºmero de veces que el modelo verÃ¡ el conjunto completo de datos de entrenamiento.
   - Cada Ã©poca representa una pasada completa a travÃ©s de todos los datos de entrenamiento.
   - Un mayor nÃºmero de Ã©pocas permite que el modelo se ajuste mejor a los datos, aunque si se excede, puede sobreajustarse. 
   - Normalmente se establecen en un valor entre 10 y 100, dependiendo del tamaÃ±o del conjunto de datos y la complejidad del modelo.
4. `validation_data=(X_val, Y_val)`: 
   `validation_data` es una tupla que contiene los datos de validaciÃ³n (entradas y etiquetas) que se usan para evaluar el rendimiento del modelo despuÃ©s de cada Ã©poca de entrenamiento.
   - Esto permite ver cÃ³mo se estÃ¡ comportando el modelo con datos que no ha visto antes, ayudando a monitorear si el modelo estÃ¡ sobreajustando.
   - Durante el entrenamiento, Keras calcula y muestra la pÃ©rdida y precisiÃ³n (u otras mÃ©tricas) en el conjunto de validaciÃ³n despuÃ©s de cada Ã©poca.

##### - Rendimiento y elecciÃ³n de HiperparÃ¡metros:
Dado que es necesario saber que hiperparÃ¡metros tomar para que la red se enetrene correctamente sin tener underfitting u overfitting. Para ello se iban a realizar varios entrenamientos con diferentes `batch_size` y `epochs` para determinar que combinaciÃ³n genera la mejor *accuracy*. Esto no se concluyÃ³ dado que el tiempo de entrenamiento es considerablemente alto. En el notebook [model_tests](supervision/notebooks/model_tests.ipynb) se pueden encontrar los resultados de 4 ejecuciones con su *accuracy*, tiempo de ejecuciÃ³n, ``batch_size`` y ``epochs``.


![EvaluaciÃ³n de HiperparÃ¡metros](./supervision/misc/evaluacion.png)


Lo ideal serÃ­a que en un servidor dejar corriendo el codigo de prueba para unos 10 a 12 casos diferentes para elegir de la mejor forma posible los hiperparÃ¡metros. Pero si el pronostico para las pruebas que realicÃ© era de 12 horas, hacerlo con tiempo y sin apuros es lo ideal.

#### - Despliegue:
Al principio la idea era desplegar una API Rest para el uso del modelo en cualquier cantidad de programas, esto se puede realizar utilizando ``fastAPI``, la cual es una librerÃ­a de python. Sin embargo, este despliegue se realizÃ³ de manera local. Esto con las mismas herramientas brindadas por las librerÃ­as de ``Tensorflow`` y ``Keras``. Esto se realiza de la siguiente forma:

1. **MÃ©todo 1:** Guardando en formato ``.h5`` que es una de las formas mÃ¡s comunes se realiza los iguiente:
   ```
   # guarda el modelo completo
   modelo.save('nombre_del_modelo.h5')

   # o de esta forma
   keras.saving.save_model(model, 'nombre_del_modelo.keras')
   ```

   - El archivo `.h5` (HDF5) contiene toda la informaciÃ³n necesaria para tu modelo:
     - La arquitectura
     - Los pesos
     - Las configuraciones de entrenamiento
     - El optimizador (si lo tienes configurado)
   - Es adecuado para la portabilidad y puede cargarse en cualquier entorno que tenga **Keras** y **TensorFlow**.
   - Tiene un tamaÃ±o moderado, dependiendo de la cantidad de pesos y la complejidad de tu modelo.
   
   Para cargar el modelo en un nuevo programa:
   ```
   from tensorflow.keras.models import load_model

   modelo = load_model('modelo_sentimiento.h5')
   ```
2. **MÃ©todo 2:** Guardar el modelo en formato de TensorFlow SavedModel. Este es el estÃ¡ndar preferido por TensorFlow, en especial cuando se desea deplegar modelos en producciÃ³n o en otros entornos de tensorflow.

   ```
   # guardar en formato SavedModel
   modelo.save('nombre_del_modelo_tf', save_format='tf')
   ```
   - El modelo se guarda en una carpeta llamada `modelo_sentimiento_tf` (o el nombre que le des) que contiene subcarpetas y archivos necesarios para TensorFlow.
   - Este formato es muy flexible y puede utilizarse para hacer despliegues en **TensorFlow Serving** o **TensorFlow Lite**.
   - Permite contener varias versiones de un mismo modelo, lo cual es Ãºtil para actualizaciones sin interrupciones.
   - Contiene tanto los pesos como la arquitectura, al igual que el formato `.h5`, pero se adapta mejor a entornos avanzados y multiplataforma de TensorFlow.

   Y para cargar el modelo:
   ```
   # cargar desde un directorio en savemodel
   modelo = load_model('nombre_del_modelo_tf')
   ```
3. **MÃ©todo 3:** Este mÃ©todo es el que menos me gusta, dado que es necesario conocer la arquitectura con la que fue creado el modelo y en caso que se desee mantener esa informaciÃ³n privada, pues deja de ser viable el mÃ©todo. Pero, el guardar unicamente los pesos hace que el archivo pese mucho menos y sea mucho mÃ¡s fÃ¡cil su despliegue en otros dispositivos.
   ```
   # Guardando los pesos
   modelo.save_weights('pesos_modelo.h5')
   ```
   - Contiene solo los pesos del modelo y no la arquitectura.
   - Es mÃ¡s liviano que guardar el modelo completo.
   - Para usarlo, necesitas reconstruir la arquitectura del modelo en el cÃ³digo antes de cargar los pesos.

   ```
   # Primero, define la arquitectura del modelo
   from tensorflow.keras.models import Sequential
   from tensorflow.keras.layers import Embedding, LSTM, Dense

   modelo = Sequential([
      #### ARQUITECTURA ####
   ])

   # Luego, cargar los pesos
   modelo.load_weights("pesos_sentimiento.h5")
   ```

| Formato       | ExtensiÃ³n | Contenido                               | Uso recomendado                              |
|---------------|-----------|-----------------------------------------|----------------------------------------------|
| .h5           | .h5       | Arquitectura + pesos + configuraciÃ³n    | FÃ¡cil para guardado local y uso en Keras.    |
| SavedModel    | Directorio| Arquitectura + pesos + metadatos en formato TF | Ideal para despliegue en TensorFlow Serving. |
| Pesos solo    | .h5       | Solo los pesos del modelo               | Para cargas personalizadas o experimentaciÃ³n. |



AdemÃ¡s es necesario guardar el tokenizador. Esto porque es un pilar fundamental en el entrenamiento del modelo y si se le asignan tokens ya usados a palabras ya existentes en el tokenizador del modelo a una palabra de lo que sea que estemos utilizando, pues ya no tendra sentido hacer predicciones. Por ende, es necesario guardar los tokens utilizados. Esto se puede hacer en formato `.json` de la siguiente forma:
```
from tensorflow.keras.preprocessing.text import Tokenizer
import json

# Supongamos que este es el tokenizador que usaste para entrenar el modelo
tokenizer = Tokenizer(num_words=10000)
tokenizer.fit_on_texts(textos_de_entrenamiento)

# Guardar el tokenizador en un archivo JSON
with open("tokenizer.json", "w") as f:
    json.dump(tokenizer.to_json(), f)
```

Y para cargar dicho tokenizador, se tiene lo siguiente:

```
from tensorflow.keras.preprocessing.text import tokenizer_from_json

# Cargar el tokenizador desde el archivo JSON
with open("tokenizer.json") as f:
    data = json.load(f)
    tokenizer = tokenizer_from_json(data)
```
Como se ve en el cÃ³digo del modelo, es necesario transformar estas palabras a ``pad_sequences``, lo cual requiere de una longitud dada, es **INDISPENSABLE** conocer este valor de longitud y utilizarlo para los nuevos textos, dado que sino es asÃ­, el modelo no podrÃ¡ procesarlo.

- **Consistencia del preprocesamiento**: AsegÃºrate de que el `maxlen` de `pad_sequences` coincida con el utilizado en el entrenamiento del modelo. De lo contrario, el modelo puede no interpretar correctamente las entradas. Esto se puede importar del archivo llamado *`maxlen`* en la carpeta de *model*.
- **TamaÃ±o del vocabulario**: AsegÃºrate de usar el mismo nÃºmero de palabras (`num_words` en `Tokenizer`) en entrenamiento e inferencia. Esto afecta a cÃ³mo se indexan y seleccionan las palabras mÃ¡s frecuentes.
- **Preprocesamiento adicional**: Si aplicaste otras transformaciones al texto (como eliminaciÃ³n de puntuaciÃ³n o minÃºsculas), aplica los mismos pasos antes de tokenizar.

### Uso del Modelo
Para utilizar las funciones de limpieza de datos/texto se requiere que en el script/notebook se importe la carpeta *source* de la siguiente forma
```
from src import *
```
Esto accesa a las clases y mÃ©todos disponibles de la carpeta `src` en el archivo `__init__.py` dado que no es necesario que se tenga acceso a todo lo disponible en dicha carpeta. 

Con esto ya se puede realizar la importaciÃ³n del modelo de algunas de las formas mostradas anteriormente. La parte mÃ¡s importante y en la que quiero hacer mÃ¡s enfasis es la parte de tokenizar el texto:
```
# cambiando el texto a secuencias para su posterior 'embedding'
x_review_tests = tokenizer.texts_to_sequences(resenias_treated['review_es_mod'])
x_review_tests = sequence.pad_sequences(x_review_tests, maxlen=maxlen, padding='post')
```

Donde el `maxlen` es extraÃ­do del archivo que lleva su nombre en la carpeta *model*.


## Datasets

Los siguientes datasets fueron utilizados en este proyecto:

1. **Dataset 1: IMDB Dataset of 50K Movie Reviews (Spanish)**  
   - Este dataset fue creado usando "[IMDB Dataset of 50K Movie Reviews](https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews)", que contiene 50k reseÃ±as de pelÃ­culas con una clasificaciÃ³n positiva o negativa a cada una de ellas. Este contiene la data original y su traducciÃ³n al espaÃ±ol.  
   - Fuente: [Kaggle - IMDB Dataset of 50K Movie Reviews (Spanish)](https://www.kaggle.com/datasets/luisdiegofv97/imdb-dataset-of-50k-movie-reviews-spanish)

2. **Dataset 2: Amazon Reviews**  
   - Este dataset contiene 34,686,770 ReseÃ±as de 6,643,669 usuarios sobre 2,441,053 productos, extraÃ­das de "Standford Network Analysis Project (SNAP)". Esto subset contiene 1,800,000 datos de entrenamiento y 200,000 datos de pruebas con polaridad en el sentimiento.
   - Fuente: [Kaggle - Amazon Reviews](https://www.kaggle.com/datasets/kritanjalijain/amazon-reviews)

3. **Dataset 3: Large Movie Review Dataset**  
   - Este conjunto de datos contiene reseÃ±as de pelÃ­culas junto con sus etiquetas de polaridad de sentimiento binario asociadas. EstÃ¡ destinado a servir como un punto de referencia para la clasificaciÃ³n de sentimientos. Este documento describe cÃ³mo se recopilÃ³ el conjunto de datos y cÃ³mo utilizar los archivos proporcionados.
   - Fuente: [Stanford - Large Movie Review Dataset](https://ai.stanford.edu/~amaas/data/sentiment/)

4. **Dataset 4: Amazon Books Reviews**
   - Este dataset contiene reseÃ±as de libros junto a sus etiquetas.
   - Fuente: [Kaggle - Amazon Books Reviews](https://www.kaggle.com/datasets/mohamedbakhet/amazon-books-reviews)



## BibliografÃ­a

1. **Rao, D., & McMahan, B.**. _Natural language processing with PyTorch: build intelligent language applications using deep learning_. O'Reilly Media, Inc., 2019.
2. **Bird, S., Klein, E., & Loper, E.**. _Natural language processing with Python: analyzing text with the natural language toolkit_. O'Reilly Media, Inc., 2009.
3. **Liu, B.**. _Sentiment analysis and opinion mining_. Springer Nature, 2022.





<!--
>[!NOTE]
Esto es una nota.

>[!TIP]
Esto es un tip.

>[!IMPORTANT]
Esto es una nota importante.

>[!WARNING]
Esto es una advertencia

>[!CAUTION]
Esto es una precauciÃ³n.
-->


